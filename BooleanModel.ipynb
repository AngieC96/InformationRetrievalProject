{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Boolean Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce  # not essential but reduces the code we have to write\n",
    "import csv     # for csv files\n",
    "import re      # for regular expressions\n",
    "import pickle  # to save the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postings\n",
    "\n",
    "A `Posting` object is simply the docID of a document. It has a method `get_from_corpus` that given the corpus retrieves the document corresponding to that docID. Then it has some comparison methods to check if two docID are equal, one greater than the other, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering   # takes a class where we have defined at least the methods `eq` and `gt`/`lt` and defines in a consistent way all the other methods (otherwise we should implement them all by hand)\n",
    "class Posting:\n",
    "    \n",
    "    def __init__(self, docID):\n",
    "        \"\"\" Class constructor.\n",
    "        \"\"\"\n",
    "        self._docID = docID\n",
    "        \n",
    "    def get_from_corpus(self, corpus):  # return from the corpus the doc corresponding to that docID. In the list you only save the docID, not the all document\n",
    "        \"\"\" Returns the document corresponding to that docID from the corpus.\n",
    "        \"\"\"\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    def __eq__(self, other: 'Posting'):  # euqality comparator\n",
    "        \"\"\" Performs the comparison between this posting and another one.\n",
    "        Since the ordering of the postings is only given by their docID,\n",
    "        they are equal when their docIDs are equal.\n",
    "        \"\"\"\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    def __gt__(self, other: 'Posting'):  # greather than comparator\n",
    "        \"\"\" As in the case of __eq__, the ordering of postings is given\n",
    "        by the ordering of their docIDs.\n",
    "        \"\"\"\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    def __repr__(self):       # for debagging purposes to print the class\n",
    "        \"\"\" String representation of the class.\n",
    "        \"\"\"\n",
    "        return str(self._docID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting Lists\n",
    "\n",
    "A `PostingList` object is a list of `Posting`s. You can construct an empty `PostingList` with `__init__`, or construct and initialize a `PostingList` directly with one docID with `from_docID`, or you can create a `PostingList` object with an already existing list using `from_posting_list`. Then you can merge two posting list with `merge` (the one in input will be added at the end of the one on which the mehod `merge` is called, without any checking on the total ordering of the list), you can intersect them with `intersection` or you can unify them with `union`. With `get_from_corpus` we can retrieve the documents corresponding to the docID stored in this `PostingList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Class constructor.\n",
    "        \"\"\"\n",
    "        self._postings = []    # it has as an attribute a list of posting\n",
    "        \n",
    "    @classmethod     # to define another constructor. It will return another PostingList like a constructor\n",
    "    def from_docID(cls, docID):\n",
    "        \"\"\" A posting list can be constructed starting from a single docID.\n",
    "        \"\"\"\n",
    "        plist = cls()\n",
    "        plist._postings = [(Posting(docID))]\n",
    "        return plist\n",
    "    \n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList: 'PostingList'):\n",
    "        \"\"\" A posting list can also be constructed by using another posting list.\n",
    "        \"\"\"\n",
    "        plist = cls()\n",
    "        plist._postings = postingList   # we use it as the postins of this PostingList\n",
    "        return plist\n",
    "    \n",
    "    def merge(self, other: 'PostingList'):  # we have to merge postinglists\n",
    "        \"\"\" Merges the other posting list to this one in a desctructive\n",
    "        way, i.e., modifying the current posting list. This method assumes\n",
    "        that all the docIDs of the second list are higher than the ones\n",
    "        in this list. It assumes the two posting lists to be ordered\n",
    "        and non-empty. Under those assumptions duplicate docIDs are\n",
    "        discarded.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        last = self._postings[-1]   # the self eleemnt of the current postinglist\n",
    "        while (i < len(other._postings) and last == other._postings[i]):  # we can have the same docID multiple times and when e merge them we don't want them multiple times\n",
    "            i += 1\n",
    "        self._postings += other._postings[i:]\n",
    "        \n",
    "    def intersection(self, other: 'PostingList'):\n",
    "        \"\"\" Returns a new posting list resulting from the intersection\n",
    "        of this one and the one passed as argument.\n",
    "        \"\"\"\n",
    "        intersection = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other._postings)):  # until we reach the end of a posting list\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingList.from_posting_list(intersection)\n",
    "    \n",
    "    def union(self, other: 'PostingList'):\n",
    "        \"\"\" Returns a new posting list resulting from the union of this\n",
    "        one and the one passed as argument.\n",
    "        \"\"\"\n",
    "        union = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other._postings)):\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                union.append(self._postings[i])   # because i is the smallest one\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j]) \n",
    "                j += 1\n",
    "        for k in range(i, len(self._postings)):  # we have to append the remaining elements of the non emptied list\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)\n",
    "    \n",
    "    def get_from_corpus(self, corpus):   # used when we have a posting list that is the result of a query, but I don't want the docID, I want the docs!\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))  # I return a list of documents\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms\n",
    "\n",
    "A `Term` object contains both the word itself and the `PostingList` with all the docIDs of the documents in which the word is contained. The `merge` function merges the `PostingList`s of two equal `Term`s. Then we have some comparison methods to check if two `Term`s are equal or one is greater then the other, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "@total_ordering  # to have all the ordering methods defined automatically\n",
    "class Term:\n",
    "    \n",
    "    def __init__(self, term, docID):   # we create a term with a DocID, we sort them and we merge the equal terms\n",
    "        self.term = term\n",
    "        self.posting_list = PostingList.from_docID(docID)\n",
    "        \n",
    "    def merge(self, other: 'Term'):   # when we merge two terms\n",
    "        \"\"\" Merges (destructively) this term and the corresponding posting list\n",
    "        with another equal term and its corrsponding posting list.\n",
    "        \"\"\"\n",
    "        if (self.term == other.term): # cannot merge posting lists with different terms!\n",
    "            self.posting_list.merge(other.posting_list)  # merge the current posting list with the one of the other\n",
    "        else: \n",
    "            raise ImpossibleMergeError # (some kind of error) error of impossible merge\n",
    "            \n",
    "    def __eq__(self, other: 'Term'):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    def __gt__(self, other: 'Term'):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do some step of tokenization and normalization\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\" A simple funzion to normalize a text.\n",
    "    It removes everything that is not a word, a space or an hyphen\n",
    "    and downcases all the text.\n",
    "    \"\"\"\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s^-]', '', text)  # the text that matches a certain pattern will be substittuted with the second expression. ^\\w → not something alphanumeric, ^\\s → not some space, ^- → not a dash, replace it with '', the empty string\n",
    "    downcase = no_punctuation.lower()  # put everything to lower case\n",
    "    return downcase\n",
    "\n",
    "def tokenize(movie: 'MovieDescription'):\n",
    "    \"\"\" Returns a list, which is a posting list, from a movie\n",
    "    description of all tokens present in the description.\n",
    "    \"\"\"\n",
    "    text = normalize(movie.description)\n",
    "    return list(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to print a progress bar, taken from [here](https://stackoverflow.com/questions/3160699/python-progress-bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "\n",
    "def update_progress(progress):\n",
    "    \"\"\" Displays or updates a console progress bar.\n",
    "    Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "    A value under 0 represents a 'halt'.\n",
    "    A value at 1 or bigger represents 100%\n",
    "    \"\"\"\n",
    "    barLength = 40 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done!\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\r[{0}] {1}% {2}\".format( \"#\"*block + \".\"*(barLength-block), round(progress*100, 2), status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `InvertedIndex` object contains a dictionary with as keys the words and as values the `Term` associated to that word, which, we reall, contains the `PostingList` associated to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dictionary = {}  # initially the inverted index dictionary is empty\n",
    "        \n",
    "    @classmethod  # instead of having this method associated to a specific instance/object of the class InvertedIndex we write InvertedIndex.from_corpus(). Because you can have only one __init__ method, so you use @classmethod to have multiple constructors. It's like a static method in Java\n",
    "    def from_corpus(cls, corpus: list):\n",
    "        intermediate_dict = {}   # we cheat a little bit and use a Python dictionary → we should create a big list, sort it and merge everything\n",
    "        print(\"Processing the corpus to create the index...\")\n",
    "        for docID, document in enumerate(corpus): # NB: corpus: collection (list) of objects of type MovieDescription\n",
    "            tokens = tokenize(document) # document is a MovieDescription object\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID)\n",
    "                try:\n",
    "                    intermediate_dict[token].merge(term)  # I merge the two posting lists → Term.merge() which calls PostingList.merge()\n",
    "                except KeyError:\n",
    "                    intermediate_dict[token] = term # for when the term is not present in the dict\n",
    "            update_progress(docID/len(corpus))  # to see the progressing of our indexing\n",
    "                \n",
    "        idx = cls()  # we call the constructor of the class = InvertedIndex\n",
    "        idx._dictionary = sorted(intermediate_dict.values())  # list of all the sorted terms\n",
    "        return idx\n",
    "    \n",
    "    def __getitem__(self, key): # indexing the inverted index using as keys the terms\n",
    "        for term in self._dictionary:  # we could do a binary search\n",
    "            if term.term == key:\n",
    "                return term.posting_list  # quering the index with a  word returns the PostingList associated to that word\n",
    "        raise KeyError(f\"The term {key} is not present in the index.\") # the key is not present!\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus\n",
    "\n",
    "A `MovieDescription` object has a title and a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class MovieDescription:  # container for all the info we have about the movie\n",
    "    \n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __eq__(self, other: 'MovieDescription'):\n",
    "        return self.title == other.title\n",
    "    \n",
    "    def __gt__(self, other: 'MovieDescription'):\n",
    "        return self.title > other.title\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_movie_descriptions():\n",
    "    filename = 'data/plot_summaries.txt'   # not very portable but done for the sake of simplicity\n",
    "    movie_names_file = 'data/movie.metadata.tsv'\n",
    "    with open(movie_names_file, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter = '\\t')   # we define the csv reader\n",
    "        names_table = {}   # Python dictionary with all the names of the films: key = movieID, value = movie title\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2] # the first element is the ID, the third elemnt is the title\n",
    "    # Now we have all the associations between ID and title, we miss the move description\n",
    "\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter = '\\t')\n",
    "        corpus = []   # collection (list) of objects of type MovieDescription\n",
    "        for desc in descriptions:\n",
    "            try:      # at least in this dataset there are some errors so some descriptions have not a matching ID\n",
    "                movie = MovieDescription(names_table[desc[0]], desc[1]) # the first element is the ID, the second the description\n",
    "                corpus.append(movie)\n",
    "            except KeyError:  # in case we don't find the title associated to that ID\n",
    "                pass\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR System\n",
    "\n",
    "An `IRsystem` object contains the entire corpus and the `InvertedIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRsystem:\n",
    "    \n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus): # generate the entire inverted index calling the constructor\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        return cls(corpus, index)  # retrun the constructor when we have yet the index\n",
    "    \n",
    "    def answer_and_query(self, words):  # For now only queries with AND. We have a list of words like ['cat', 'batman']\n",
    "        norm_words = map(normalize, words)  # Normalize all the words. IMPORTANT!!! If the user uses upper-case we will not have ANY match! We have to perform the same normalization of the docs in the corpus on the query!\n",
    "        postings = map(lambda w: self._index[w], norm_words) # get the posting list for each word → list of posting lists\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)  # apply the function to the two items of the list, then apply it to the result with the third, then the result with the fourt term and so on until the end of the list\n",
    "        return plist.get_from_corpus(self._corpus) # retrun the documents\n",
    "    \n",
    "    def answer_or_query(self, words):\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = map(lambda w: self._index[w], norm_words)\n",
    "        plist = reduce(lambda x, y: x.union(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_query(ir, text, noprint=True):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_and_query(words)  # list of documents\n",
    "    if not noprint:\n",
    "        for movie in answer:\n",
    "            print(movie)\n",
    "    return answer\n",
    "        \n",
    "def or_query(ir, text, noprint=True):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_or_query(words)\n",
    "    if not noprint:\n",
    "        for movie in answer:\n",
    "            print(movie)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PostingList.from_docID(10)   # creates a PostingL with the docID 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 3, 6\n"
     ]
    }
   ],
   "source": [
    "x = Term(\"cat\", 3)\n",
    "y = Term(\"cat\", 6)\n",
    "x.merge(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = Posting(1)\n",
    "y = Posting(2)\n",
    "print(x < y)\n",
    "print(x <= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eg this is a test'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(\"e.g., this is A TeSt\") # we remove punctuation and put everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'movie', 'description']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(MovieDescription(\"Title\", \"This is a movie Description.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_movie_descriptions()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = InvertedIndex.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dictionary with 194757 terms\n"
     ]
    }
   ],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334, 2990, 3463, 3519, 3545, 5510, 6854, 7105, 7358, 8467, 9503, 10360, 10727, 10933, 12458, 12492, 12967, 13095, 14199, 17366, 18875, 19381, 19675, 20598, 20808, 21070, 22147, 24393, 24484, 24658, 25866, 30601, 31272, 31508, 33213, 33638, 35356, 35980, 37238, 37389, 38152, 39092, 40499, 40596, 40821"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx['batman']  # all the docIDs containing 'batman' in the description!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = IRsystem(corpus, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The Lord of the Rings: The Fellowship of the Ring,\n",
       " The Lord of the Rings,\n",
       " The Hunt for Gollum,\n",
       " The Return of the King,\n",
       " Date Movie,\n",
       " The Lord of the Rings: The Two Towers,\n",
       " The Lord of the Rings: The Return of the King]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_query(ir, \"frodo Gandalf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ciao'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    and_query(ir, \"thig\")\n",
    "except KeyError:\n",
    "    print(sys.exc_info()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Star Wars Episode V: The Empire Strikes Back,\n",
       " Something, Something, Something Dark Side,\n",
       " Return of the Ewok,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Star Wars Episode VI: Return of the Jedi,\n",
       " It's a Trap!]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_query(ir, \"yoda luke darth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Star Wars Episode V: The Empire Strikes Back,\n",
       " Star Wars Episode II: Attack of the Clones,\n",
       " George Lucas in Love,\n",
       " The Lord of the Rings: The Fellowship of the Ring,\n",
       " The Lord of the Rings,\n",
       " Something, Something, Something Dark Side,\n",
       " The Hunt for Gollum,\n",
       " The Return of the King,\n",
       " Return of the Ewok,\n",
       " Aliens in the Wild, Wild West,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Star Wars Episode VI: Return of the Jedi,\n",
       " Star Wars: The Clone Wars,\n",
       " Date Movie,\n",
       " Gulliver's Travels,\n",
       " Lego Star Wars: The Quest for R2-D2,\n",
       " The Lord of the Rings: The Two Towers,\n",
       " It's a Trap!,\n",
       " The Lord of the Rings: The Return of the King,\n",
       " LEGO Star Wars: Revenge of the Brick]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_query(ir, \"frodo yoda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frodo_query = and_query(ir, \"frodo\")\n",
    "yoda_query = and_query(ir, \"yoda\")\n",
    "frodo_or_yoda_query = or_query(ir, \"frodo yoda\")\n",
    "# frodo_query.extend(yoda_query)  # then print 'frodo_query' !!!!\n",
    "frodo_p_yoda_query = frodo_query + yoda_query\n",
    "assert sorted(frodo_p_yoda_query) == sorted(frodo_or_yoda_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique(original_list):\n",
    "    unique_list = []\n",
    "    [unique_list.append(obj) for obj in original_list if obj not in unique_list]\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 13 8\n",
      "28 21 21\n",
      "[Date Movie, The Hunt for Gollum, The Lord of the Rings, The Lord of the Rings: The Fellowship of the Ring, The Lord of the Rings: The Return of the King, The Lord of the Rings: The Two Towers, The Return of the King]\n",
      "-----------\n",
      "[Aliens in the Wild, Wild West, George Lucas in Love, Gulliver's Travels, It's a Trap!, LEGO Star Wars: Revenge of the Brick, Lego Star Wars: The Quest for R2-D2, Return of the Ewok, Something, Something, Something Dark Side, Star Wars Episode II: Attack of the Clones, Star Wars Episode III: Revenge of the Sith, Star Wars Episode V: The Empire Strikes Back, Star Wars Episode VI: Return of the Jedi, Star Wars: The Clone Wars]\n",
      "-----------\n",
      "[Date Movie, Imaginationland Episode II, The Hunt for Gollum, The Lord of the Rings, The Lord of the Rings: The Fellowship of the Ring, The Lord of the Rings: The Return of the King, The Lord of the Rings: The Two Towers, The Return of the King]\n",
      "-----------\n",
      "[Aliens in the Wild, Wild West, Date Movie, George Lucas in Love, Gulliver's Travels, Imaginationland Episode II, It's a Trap!, LEGO Star Wars: Revenge of the Brick, Lego Star Wars: The Quest for R2-D2, Return of the Ewok, Something, Something, Something Dark Side, Star Wars Episode II: Attack of the Clones, Star Wars Episode III: Revenge of the Sith, Star Wars Episode V: The Empire Strikes Back, Star Wars Episode VI: Return of the Jedi, Star Wars: The Clone Wars, The Hunt for Gollum, The Lord of the Rings, The Lord of the Rings: The Fellowship of the Ring, The Lord of the Rings: The Return of the King, The Lord of the Rings: The Two Towers, The Return of the King]\n"
     ]
    }
   ],
   "source": [
    "frodo_query = and_query(ir, \"frodo\")\n",
    "yoda_query = and_query(ir, \"yoda\")\n",
    "third_query = and_query(ir, \"gandalf\")\n",
    "my_or_query = sorted(or_query(ir, \"frodo yoda gandalf\"))\n",
    "all_query = sorted(frodo_query + yoda_query + third_query)\n",
    "\n",
    "#all_query = make_unique(all_query)\n",
    "print(len(frodo_query), len(yoda_query), len(third_query))\n",
    "print(len(all_query), len(make_unique(all_query)), len(my_or_query))\n",
    "\n",
    "print(sorted(frodo_query))\n",
    "print(\"-----------\")\n",
    "print(sorted(yoda_query))\n",
    "print(\"-----------\")\n",
    "print(sorted(third_query))\n",
    "print(\"-----------\")\n",
    "print(sorted(my_or_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 13 10461\n",
      "10481 10474\n",
      "10175 10175\n"
     ]
    }
   ],
   "source": [
    "frodo_query = and_query(ir, \"frodo\")\n",
    "yoda_query = and_query(ir, \"yoda\")\n",
    "third_query = and_query(ir, \"love\")\n",
    "my_or_query = sorted(or_query(ir, \"frodo yoda love\"))\n",
    "all_query = sorted(frodo_query + yoda_query + third_query)\n",
    "#all_query = make_unique(all_query)\n",
    "\n",
    "#print(sorted(frodo_query))\n",
    "#print(\"-----------\")\n",
    "#print(sorted(yoda_query))\n",
    "#print(\"-----------\")\n",
    "#print(sorted(catwoman_query))\n",
    "#print(\"-----------\")\n",
    "#print(sorted(my_or_query))\n",
    "\n",
    "print(len(frodo_query), len(yoda_query), len(third_query))\n",
    "print(len(all_query), len(my_or_query))\n",
    "\n",
    "all_query_uniq = make_unique(all_query)\n",
    "my_or_query_unique = make_unique(my_or_query)\n",
    "print(len(all_query_uniq), len(my_or_query_unique))\n",
    "\n",
    "assert sorted(all_query_uniq) == sorted(my_or_query_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_query_uniq)):\n",
    "    if all_query_uniq[i] != my_or_query_unique[i]:\n",
    "        print(f\"{all_query_uniq[i-1].title:<30}\\t --  {my_or_query_unique[i-1]}\")\n",
    "        print(f\"{all_query_uniq[i].title:<30}\\t --  {my_or_query_unique[i]}\")\n",
    "        print(f\"{all_query_uniq[i+1].title:<30}\\t --  {my_or_query_unique[i+1]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Movie                    \t --  Date Movie\n",
      "Date Movie                    \t --  Date Night\n",
      "Date Night                    \t --  Daughter of the East\n",
      "\n",
      "Second missing word:\n",
      "Lego Star Wars: The Quest for R2-D2\t --  Lego Star Wars: The Quest for R2-D2\n",
      "Lego Star Wars: The Quest for R2-D2\t --  Lekhayude Maranam Oru Flashback\n",
      "Lekhayude Maranam Oru Flashback\t --  Lemonade Joe\n",
      "[Date Movie, Lego Star Wars: The Quest for R2-D2]\n"
     ]
    }
   ],
   "source": [
    "errors = {'love': 7, 'mother': 2, 'father': 7, 'cat': 1, 'me': 1}\n",
    "\n",
    "missing_titles = []\n",
    "for i in range(len(my_or_query)):\n",
    "    if all_query[i] != my_or_query[i]:\n",
    "        missing_titles.append(all_query[i])\n",
    "        print(f\"{all_query[i-1].title:<30}\\t --  {my_or_query[i-1]}\")\n",
    "        print(f\"{all_query[i].title:<30}\\t --  {my_or_query[i]}\")\n",
    "        print(f\"{all_query[i+1].title:<30}\\t --  {my_or_query[i+1]}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nSecond missing word:\")\n",
    "for j in range(i+1,len(my_or_query)):\n",
    "    if all_query[j] != my_or_query[j-1]:\n",
    "        missing_titles.append(all_query[j])\n",
    "        print(f\"{all_query[j-1].title:<30}\\t --  {my_or_query[j-2]}\")\n",
    "        print(f\"{all_query[j].title:<30}\\t --  {my_or_query[j-1]}\")\n",
    "        print(f\"{all_query[j+1].title:<30}\\t --  {my_or_query[j]}\")\n",
    "        break\n",
    "\n",
    "print(missing_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Date Movie in 34599 → Date Movie\n",
      "\n",
      "my_or_query\n",
      "Found: Date Movie in 1413 → Date Movie\n",
      "\n",
      "all_query\n",
      "Found: Date Movie in 1413 → Date Movie\n",
      "Found: Date Movie in 1414 → Date Movie\n",
      "\n",
      "frodo_query\n",
      "Found: Date Movie in 4 → Date Movie\n",
      "\n",
      "yoda_query\n",
      "\n",
      "thirs_query\n",
      "Found: Date Movie in 5780 → Date Movie\n"
     ]
    }
   ],
   "source": [
    "for i, movie in enumerate(corpus):\n",
    "    if movie == missing_titles[0]:\n",
    "    # if movie.title == missing_titles[0].title:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {corpus[i]}\")\n",
    "\n",
    "print(\"\\nmy_or_query\")\n",
    "for i, movie in enumerate(my_or_query):\n",
    "    if movie == missing_titles[0]:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {my_or_query[i]}\")\n",
    "\n",
    "print(\"\\nall_query\")\n",
    "for i, movie in enumerate(all_query):\n",
    "    if movie == missing_titles[0]:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {all_query[i]}\")\n",
    "        \n",
    "print(\"\\nfrodo_query\")\n",
    "for i, movie in enumerate(frodo_query):\n",
    "    if movie == missing_titles[0]:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {frodo_query[i]}\")\n",
    "        \n",
    "print(\"\\nyoda_query\")\n",
    "for i, movie in enumerate(yoda_query):\n",
    "    if movie == missing_titles[0]:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {yoda_query[i]}\")\n",
    "\n",
    "print(\"\\nthirs_query\")\n",
    "for i, movie in enumerate(third_query):\n",
    "    if movie == missing_titles[0]:\n",
    "        print(f\"Found: {missing_titles[0].title} in {i} → {third_query[i]}\")\n",
    "        \n",
    "print(len(make_unique(all_query)), len(make_unique(my_or_query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Date Movie in 34599 → Date Movie\n",
      "\n",
      "my_or_query\n",
      "Found: Date Movie in 1413 → Date Movie\n",
      "\n",
      "all_query\n",
      "Found: Date Movie in 1413 → Date Movie\n",
      "Found: Date Movie in 1414 → Date Movie\n",
      "6933 6933\n",
      "42204 39914\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus), len(make_unique(corpus)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}