{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "BooleanModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iKBPgxS2xGi_",
        "FyQ757d1xGjD",
        "j2CDYrbSxGjh",
        "TCTOmEL6xGj2",
        "y41lObHswkdp"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UC3PFRExGi5"
      },
      "source": [
        "# A Boolean Retrieval System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGEqloc9xGi9"
      },
      "source": [
        "from functools import total_ordering, reduce  # not essential but reduces the code we have to write\n",
        "import csv     # for csv files\n",
        "import re      # for regular expressions\n",
        "import pickle  # to save the index\n",
        "import time\n",
        "import os.path\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKBPgxS2xGi_"
      },
      "source": [
        "## Postings\n",
        "\n",
        "A `Posting` object is simply the docID of a document. It has a method `get_from_corpus` that given the corpus retrieves the document corresponding to that docID. Then it has some comparison methods to check if two docID are equal, one greater than the other, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ECFCVZdxGjA"
      },
      "source": [
        "@total_ordering   # takes a class where we have defined at least the methods `eq` and `gt`/`lt` and defines in a consistent way all the other methods (otherwise we should implement them all by hand)\n",
        "class Posting:\n",
        "    \n",
        "    def __init__(self, docID):\n",
        "        \"\"\" Class constructor.\n",
        "        \"\"\"\n",
        "        self._docID = docID\n",
        "        \n",
        "    def get_from_corpus(self, corpus):  # return from the corpus the doc corresponding to that docID. In the list you only save the docID, not the all document\n",
        "        \"\"\" Returns the document corresponding to that docID from the corpus.\n",
        "        \"\"\"\n",
        "        return corpus[self._docID]\n",
        "    \n",
        "    def __eq__(self, other: 'Posting'):  # euqality comparator\n",
        "        \"\"\" Performs the comparison between this posting and another one.\n",
        "        Since the ordering of the postings is only given by their docID,\n",
        "        they are equal when their docIDs are equal.\n",
        "        \"\"\"\n",
        "        return self._docID == other._docID\n",
        "    \n",
        "    def __gt__(self, other: 'Posting'):  # greather than comparator\n",
        "        \"\"\" As in the case of __eq__, the ordering of postings is given\n",
        "        by the ordering of their docIDs.\n",
        "        \"\"\"\n",
        "        return self._docID > other._docID\n",
        "    \n",
        "    def __repr__(self):       # for debagging purposes to print the class\n",
        "        \"\"\" String representation of the class.\n",
        "        \"\"\"\n",
        "        return str(self._docID)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyQ757d1xGjD"
      },
      "source": [
        "## Posting Lists\n",
        "\n",
        "A `PostingList` object is a list of `Posting`s. You can construct an empty `PostingList` with `__init__`, or construct and initialize a `PostingList` directly with one docID with `from_docID`, or you can create a `PostingList` object with an already existing list using `from_posting_list`. Then you can merge two posting list with `merge` (the one in input will be added at the end of the one on which the mehod `merge` is called, without any checking on the total ordering of the list), you can intersect them with `intersection` or you can unify them with `union`. With `get_from_corpus` we can retrieve the documents corresponding to the docID stored in this `PostingList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VSK6GQvxGjE"
      },
      "source": [
        "class PostingList:\n",
        "\n",
        "    _postings: list\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\" Class constructor.\n",
        "        \"\"\"\n",
        "        self._postings = []    # it has as an attribute a list of posting\n",
        "        \n",
        "    @classmethod     # to define another constructor. It will return another PostingList like a constructor\n",
        "    def from_docID(cls, docID):\n",
        "        \"\"\" A posting list can be constructed starting from a single docID.\n",
        "        \"\"\"\n",
        "        plist = cls()\n",
        "        plist._postings = [(Posting(docID))]\n",
        "        return plist\n",
        "    \n",
        "    @classmethod\n",
        "    def from_posting_list(cls, postingList):\n",
        "        \"\"\" A posting list can also be constructed by using another posting list.\n",
        "        \"\"\"\n",
        "        plist = cls()\n",
        "        plist._postings = postingList   # we use it as the postins of this PostingList\n",
        "        return plist\n",
        "    \n",
        "    def merge(self, other: 'PostingList'):  # we have to merge postinglists\n",
        "        \"\"\" Merges the other posting list to this one in a desctructive\n",
        "        way, i.e., modifying the current posting list. This method assumes\n",
        "        that all the docIDs of the second list are higher than the ones\n",
        "        in this list. It assumes the two posting lists to be ordered\n",
        "        and non-empty. Under those assumptions duplicate docIDs are\n",
        "        discarded.\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        last = self._postings[-1]   # the self element of the current postinglist\n",
        "        while (i < len(other._postings) and last == other._postings[i]):  # we can have the same docID multiple times and when e merge them we don't want them multiple times\n",
        "            i += 1\n",
        "        self._postings += other._postings[i:]\n",
        "        \n",
        "    def intersection(self, other: 'PostingList'):\n",
        "        \"\"\" Returns a new posting list resulting from the intersection\n",
        "        of this one and the one passed as argument.\n",
        "        \"\"\"\n",
        "        intersection = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while (i < len(self._postings) and j < len(other._postings)):  # until we reach the end of a posting list\n",
        "            if (self._postings[i] == other._postings[j]):\n",
        "                intersection.append(self._postings[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif (self._postings[i] < other._postings[j]):\n",
        "                i += 1\n",
        "            else:\n",
        "                j += 1\n",
        "        return PostingList.from_posting_list(intersection)\n",
        "    \n",
        "    def union(self, other: 'PostingList'):\n",
        "        \"\"\" Returns a new posting list resulting from the union of this\n",
        "        one and the one passed as argument.\n",
        "        \"\"\"\n",
        "        union = []\n",
        "        i = 0\n",
        "        j = 0\n",
        "        while (i < len(self._postings) and j < len(other._postings)):\n",
        "            if (self._postings[i] == other._postings[j]):\n",
        "                union.append(self._postings[i])\n",
        "                i += 1\n",
        "                j += 1\n",
        "            elif (self._postings[i] < other._postings[j]):\n",
        "                union.append(self._postings[i])   # because i is the smallest one\n",
        "                i += 1\n",
        "            else:\n",
        "                union.append(other._postings[j]) \n",
        "                j += 1\n",
        "        for k in range(i, len(self._postings)):  # we have to append the remaining elements of the non emptied list\n",
        "            union.append(self._postings[k])\n",
        "        for k in range(j, len(other._postings)):\n",
        "            union.append(other._postings[k])\n",
        "        return PostingList.from_posting_list(union)\n",
        "\n",
        "    def difference(self, other: 'PostingList'):\n",
        "      difference = []\n",
        "\n",
        "      return PostingList.from_posting_list(difference)\n",
        "    \n",
        "    def get_from_corpus(self, corpus):   # used when we have a posting list that is the result of a query, but I don't want the docID, I want the docs!\n",
        "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))  # I return a list of documents\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        return self._postings[key]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._postings)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \", \".join(map(str, self._postings))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2CDYrbSxGjh"
      },
      "source": [
        "## Terms\n",
        "\n",
        "A `Term` object contains both the word itself and the `PostingList` with all the docIDs of the documents in which the word is contained. The `merge` function merges the `PostingList`s of two equal `Term`s. Then we have some comparison methods to check if two `Term`s are equal or one is greater then the other, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA4u5J7LxGji"
      },
      "source": [
        "class ImpossibleMergeError(Exception):\n",
        "    pass\n",
        "\n",
        "@total_ordering  # to have all the ordering methods defined automatically\n",
        "class Term:\n",
        "\n",
        "    posting_list: PostingList\n",
        "    \n",
        "    def __init__(self, term, docID):   # we create a term with a DocID, we sort them and we merge the equal terms\n",
        "        self.term = term\n",
        "        self.posting_list = PostingList.from_docID(docID)\n",
        "        \n",
        "    def merge(self, other: 'Term'):   # when we merge two terms\n",
        "        \"\"\" Merges (destructively) this term and the corresponding posting list\n",
        "        with another equal term and its corrsponding posting list.\n",
        "        \"\"\"\n",
        "        if (self.term == other.term): # cannot merge posting lists with different terms!\n",
        "            self.posting_list.merge(other.posting_list)  # merge the current posting list with the one of the other\n",
        "        else: \n",
        "            raise ImpossibleMergeError # (some kind of error) error of impossible merge\n",
        "            \n",
        "    def __eq__(self, other: 'Term'):\n",
        "        return self.term == other.term\n",
        "    \n",
        "    def __gt__(self, other: 'Term'):\n",
        "        return self.term > other.term\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.term + \": \" + repr(self.posting_list)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3LFUWhbxGjj"
      },
      "source": [
        "## Inverted Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCbGAP0HxGjk"
      },
      "source": [
        "# We have to do some step of tokenization and normalization\n",
        "\n",
        "def normalize(text):\n",
        "    \"\"\" A simple funzion to normalize a text.\n",
        "    It removes everything that is not a word, a space or an hyphen\n",
        "    and downcases all the text.\n",
        "    \"\"\"\n",
        "    no_punctuation = re.sub(r'[^\\w^\\s^-]', '', text)  # the text that matches a certain pattern will be substittuted with the second expression. ^\\w → not something alphanumeric, ^\\s → not some space, ^- → not a dash, replace it with '', the empty string\n",
        "    downcase = no_punctuation.lower()  # put everything to lower case\n",
        "    return downcase\n",
        "\n",
        "def tokenize(movie: 'MovieDescription'):\n",
        "    \"\"\" Returns a list, which is a posting list, from a movie\n",
        "    description of all tokens present in the description.\n",
        "    \"\"\"\n",
        "    text = normalize(movie.description)\n",
        "    return list(text.split())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2P799YzxGjm"
      },
      "source": [
        "Function to print a progress bar, taken from [here](https://stackoverflow.com/questions/3160699/python-progress-bar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Xxb6GWxGjm"
      },
      "source": [
        "import time, sys\n",
        "\n",
        "def update_progress(progress):\n",
        "    \"\"\" Displays or updates a console progress bar.\n",
        "    Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "    A value under 0 represents a 'halt'.\n",
        "    A value at 1 or bigger represents 100%\n",
        "    \"\"\"\n",
        "    barLength = 40 # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done!\\r\\n\"\n",
        "    block = int(round(barLength*progress))\n",
        "    text = \"\\r[{0}] {1}% {2}\".format( \"#\"*block + \".\"*(barLength-block), round(progress*100, 2), status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eEWmsg7xGjn"
      },
      "source": [
        "In an inverted index we store, for each term, the list of documents containing it.\n",
        "\n",
        "So an `InvertedIndex` object contains a dictionary `_dictionary` with as keys the words themselves and as values the `Term` associated to each word, which, we recall, contains the `PostingList` associated to the word.\\\n",
        "It also stores a list with all the Postings, `complete_plist`, used to answer the NOT queries.\n",
        "\n",
        "\n",
        "#### Phrase queries\n",
        "\n",
        "Answering a phrase query with positional indexing\n",
        "We perform something like the intersection only that now we have to go inside and check the\n",
        "position.\n",
        "We need to check if the two terms appear in adjacent positions → We search if they are contained\n",
        "in the same document and if they are one after the other.\n",
        "\n",
        "#### Data structure\n",
        "\n",
        "Python dictionaries aren’t always what you need: the most important case is where you want to store a very large mapping. When a Python dictionary is accessed, the whole dictionary has to be unpickled and brought into memory.\n",
        "\n",
        "BTrees are a balanced tree data structure that behave like a binary tree but distribute keys throughout a number of tree nodes and each node has between $a$ and $b$ children. The nodes are stored in sorted order. Nodes are then only unpickled and brought into memory as they’re accessed, so the entire tree doesn’t have to occupy memory (unless you really are touching every single key)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tQstca6xGjz"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "class InvertedIndex:\n",
        "\n",
        "    _dictionary: List\n",
        "    complete_plist: PostingList\n",
        "    \n",
        "    def __init__(self):\n",
        "        self._dictionary = []\n",
        "        self.complete_plist = PostingList() # PostingList of all the documents\n",
        "        \n",
        "    @classmethod  # instead of having this method associated to a specific instance/object of the class InvertedIndex we write InvertedIndex.from_corpus(). Because you can have only one __init__ method, so you use @classmethod to have multiple constructors. It's like a static method in Java\n",
        "    def from_corpus(cls, corpus: list):\n",
        "        # Here we \"cheat\" by using python dictionaries\n",
        "        intermediate_dict = {}   # we cheat a little bit and use a Python dictionary → we should create a big list, sort it and merge everything\n",
        "        print(\"Processing the corpus to create the index...\")\n",
        "        for docID, document in enumerate(corpus): # NB: corpus: collection (list) of objects of type MovieDescription\n",
        "            if docID == 0:\n",
        "                plist = PostingList.from_docID(docID)\n",
        "            else:\n",
        "                plist.merge(PostingList.from_docID(docID)) # I update the PostingList of all the docs\n",
        "            tokens = tokenize(document) # document is a MovieDescription object\n",
        "            for token in tokens:\n",
        "                term = Term(token, docID)\n",
        "                try:\n",
        "                    intermediate_dict[token].merge(term)  # I merge the two posting lists → Term.merge() which calls PostingList.merge()\n",
        "                except KeyError:\n",
        "                    intermediate_dict[token] = term # for when the term is not present in the dict\n",
        "            # To observe the progressing of our indexing\n",
        "            update_progress(docID/len(corpus))\n",
        "        \n",
        "        idx = cls()  # we call the constructor of the class = InvertedIndex\n",
        "        idx._dictionary = sorted(intermediate_dict.values())  # list of all the sorted terms\n",
        "        idx.complete_plist = plist\n",
        "        return idx\n",
        "    \n",
        "    def __getitem__(self, key): # indexing the inverted index using as keys the terms\n",
        "        for term in self._dictionary:  # we could do a binary search\n",
        "            if term.term == key:\n",
        "                return term.posting_list  # quering the index with a  word returns the PostingList associated to that word\n",
        "        raise KeyError(f\"The term '{key}' is not present in the index.\") # the key is not present!\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCTOmEL6xGj2"
      },
      "source": [
        "## Reading the Corpus\n",
        "\n",
        "A `MovieDescription` object has a title and a description.  We have some comparison methods to check if two `MovieDescription`s are equal or one is greater then the other, etc. The function `hash` computes the hash of a `MovieDescription` using the hash of its title and its description.\n",
        "\n",
        "We have implemented the comparison methods to make `MovieDescription` a sortable object (so we can iterate on it), and the `hash` method to make it hashable (so we can put it in a `set`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQCbEDYixGj3"
      },
      "source": [
        "@total_ordering\n",
        "class MovieDescription:  # container for all the info we have about the movie\n",
        "    \n",
        "    def __init__(self, title: str, description: str):\n",
        "        self.title = title\n",
        "        self.description = description\n",
        "        \n",
        "    def __eq__(self, other: 'MovieDescription'):\n",
        "        return self.title == other.title\n",
        "    \n",
        "    def __gt__(self, other: 'MovieDescription'):\n",
        "        return self.title > other.title\n",
        "\n",
        "    def __hash__(self):\n",
        "      return hash((self.title, self.description))\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.title  # + \"\\n\" + self.description + \"\\n\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3jYwnNzxGj4"
      },
      "source": [
        "def read_movie_descriptions():\n",
        "    filename = 'data/plot_summaries.txt'   # not very portable but done for the sake of simplicity\n",
        "    movie_names_file = 'data/movie.metadata.tsv'\n",
        "    with open(movie_names_file, 'r') as csv_file:\n",
        "        movie_names = csv.reader(csv_file, delimiter = '\\t')   # we define the csv reader\n",
        "        names_table = {}   # Python dictionary with all the names of the films: key = movieID, value = movie title\n",
        "        for name in movie_names:\n",
        "            names_table[name[0]] = name[2] # the first element is the ID, the third elemnt is the title\n",
        "    # Now we have all the associations between ID and title, we miss the move description\n",
        "\n",
        "    with open(filename, 'r') as csv_file:\n",
        "        descriptions = csv.reader(csv_file, delimiter = '\\t')\n",
        "        corpus = []   # collection (list) of objects of type MovieDescription\n",
        "        for desc in descriptions:\n",
        "            try:      # at least in this dataset there are some errors so some descriptions have not a matching ID\n",
        "                movie = MovieDescription(names_table[desc[0]], desc[1]) # the first element is the ID, the second the description\n",
        "                corpus.append(movie)\n",
        "            except KeyError:  # in case we don't find the title associated to that ID\n",
        "                # We ignore the descriptions for which we cannot find a title\n",
        "                pass\n",
        "        return corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y41lObHswkdp"
      },
      "source": [
        "## Edit distance\n",
        "\n",
        "By computing the edit distance we can find the set of words that are the closest to a misspelled word. However, computing the edit distance on the entire dictionary can be too expensive. We can use some heuristics to limit the number of words, like looking only at words with the same initial letter (hopefully this has not been misspelled). The latter is what is implemented in this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTRU_Ar2wj4Q"
      },
      "source": [
        "def edit_distance(u, v, print = False):\n",
        "    \"\"\" Computes the edit (or Levenshtein) distance between two words u and v.\n",
        "    \"\"\"\n",
        "    nrows = len(u) + 1\n",
        "    ncols = len(v) + 1\n",
        "    M = [[0] * ncols for i in range(0, nrows)]  # matrix all filled with zeros\n",
        "    for i in range(0, nrows):  # we fill the first row, the trivial one\n",
        "        M[i][0] = i\n",
        "    for j in range(0, ncols):  # we fill the first col, the trivial one\n",
        "        M[0][j] = j\n",
        "    for i in range(1, nrows):\n",
        "        for j in range(1, ncols):\n",
        "            candidates = [M[i-1][j] + 1, M[i][j-1] + 1]\n",
        "            if (u[i-1] == v[j-1]):\n",
        "                candidates.append(M[i-1][j-1])\n",
        "            else:\n",
        "                candidates.append(M[i-1][j-1] + 1)\n",
        "            M[i][j] = min(candidates)\n",
        "            # To print the distance matrix\n",
        "            if print:\n",
        "                print(M[i][j], end=\"\\t\")\n",
        "        if print:\n",
        "          print()\n",
        "    return M[-1][-1]  # Bottom right element of M (-1 means the last element)\n",
        "\n",
        "def find_nearest(word, dictionary, keep_first=False):\n",
        "    if keep_first:\n",
        "        # If keep_first is true then we only search across the words in the dictionary starting with the same letter\n",
        "        dictionary = [w for w in dictionary if w[0] == word[0]]\n",
        "    # Remove comment to see the reduction in the size of the dictionary when keeping fixed the first letter\n",
        "    #print(len(dictionary))\n",
        "    # Apply f(x) = edit_distance(word, x) to all words in the dictionary\n",
        "    distances = map(lambda x: edit_distance(word, x), dictionary)\n",
        "    # Produce all the pairs (distance, term) usng zip and find one with the minimal distance.\n",
        "    return min(zip(distances, dictionary))[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hDkJdmCxGkH"
      },
      "source": [
        "## IR System\n",
        "\n",
        "An `IRsystem` object contains the entire corpus and the `InvertedIndex`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI9fPG7JxGkI"
      },
      "source": [
        "class IRsystem:\n",
        "\n",
        "    _corpus: list\n",
        "    _index: InvertedIndex\n",
        "    \n",
        "    def __init__(self, corpus: list, index: 'InvertedIndex'):\n",
        "        self._corpus = corpus\n",
        "        self._index = index\n",
        "        \n",
        "    @classmethod\n",
        "    def from_corpus(cls, corpus: list): # generate the entire inverted index calling the constructor\n",
        "        index = InvertedIndex.from_corpus(corpus)\n",
        "        return cls(corpus, index)  # retrun the constructor when we have yet the index\n",
        "\n",
        "    def get_from_corpus(self, plist):\n",
        "        return plist.get_from_corpus(self._corpus)\n",
        "\n",
        "    def spelling_correction(self, norm_words: List[str]):\n",
        "        postings = []\n",
        "        for w in norm_words:\n",
        "            try:\n",
        "                res = self._index[w]\n",
        "            except KeyError:\n",
        "                dictionary = [t.term for t in self._index._dictionary]\n",
        "                sub = find_nearest(w, dictionary, keep_first=True)\n",
        "                print(\"{} not found. Did you mean {}?\".format(w, sub))\n",
        "                res = self._index[sub]\n",
        "            postings.append(res)\n",
        "        return postings\n",
        "\n",
        "    def answer_and_query(self, words: List[str], spellingCorrection = False):\n",
        "        \"\"\" AND-query, if `spellingCorrection` is `True` with spelling correction\n",
        "        \"\"\"\n",
        "        norm_words = map(normalize, words)  # Normalize all the words. IMPORTANT!!! If the user uses upper-case we will not have ANY match! We have to perform the same normalization of the docs in the corpus on the query!\n",
        "        if not spellingCorrection:\n",
        "            postings = map(lambda w: self._index[w], norm_words) # get the posting list for each word → list of posting lists\n",
        "        else:\n",
        "            postings = self.spelling_correction(norm_words)\n",
        "        plist = reduce(lambda x, y: x.intersection(y), postings)  # apply the function to the two items of the list, then apply it to the result with the third, then the result with the fourt term and so on until the end of the list\n",
        "        return self.get_from_corpus(plist)\n",
        "\n",
        "    def answer_or_query(self, words: List[str], spellingCorrection = False):\n",
        "        \"\"\" OR-query, if `spellingCorrection` is `True` with spelling correction\n",
        "        \"\"\"\n",
        "        norm_words = map(normalize, words)\n",
        "        if not spellingCorrection:\n",
        "            postings = map(lambda w: self._index[w], norm_words)\n",
        "        else:\n",
        "            postings = self.spelling_correction(norm_words)\n",
        "        plist = reduce(lambda x, y: x.union(y), postings)\n",
        "        return self.get_from_corpus(plist)\n",
        "\n",
        "    def answer_not_query(self, words: List[str], spellingCorrection = False):\n",
        "        \"\"\" NOT-query (if `words` is longer than 1, the words are connected using an AND and then the NOT is performed)\n",
        "        If `spellingCorrection` is `True` with spelling correction\n",
        "        \"\"\"\n",
        "        norm_words = map(normalize, words)\n",
        "        if not spellingCorrection:\n",
        "            postings = map(lambda w: self._index[w], norm_words)\n",
        "        else:\n",
        "            postings = self.spelling_correction(norm_words)\n",
        "        words_plist = reduce(lambda x, y: x.union(y), postings)\n",
        "        plist = copy.deepcopy(self._index.complete_plist)\n",
        "        for i in words_plist:\n",
        "            if i in plist:\n",
        "                plist._postings.remove(i)\n",
        "        return self.get_from_corpus(plist)\n",
        "\n",
        "    def answer_query(self, op: str, words = None, word = None, postings = None, postings2 = None, NOT_switch = False, spellingCorrection = False):\n",
        "        ''' Complex query.\n",
        "        Arguments:\n",
        "          NOT_switch -- Used to switch the order of the two posting lists `postings` and `postings2` in the NOT query, since this operator is not commutative.\n",
        "          spellingCorrection -- if `True` the spelling correction is performed\n",
        "        '''\n",
        "        if words:\n",
        "            norm_word_0 = normalize(words[0])\n",
        "            norm_word_1 = normalize(words[1])\n",
        "            if not spellingCorrection:\n",
        "                postings = self._index[norm_word_0]\n",
        "                postings2 = self._index[norm_word_1]\n",
        "            else:\n",
        "                postings = self.spelling_correction([norm_word_0])[0]\n",
        "                postings2 = self.spelling_correction([norm_word_1])[0]\n",
        "        elif word:\n",
        "            norm_word = normalize(word)\n",
        "            if not spellingCorrection:\n",
        "                postings2 = self._index[norm_word]\n",
        "            else:\n",
        "                postings2 = self.spelling_correction([norm_word])[0]\n",
        "\n",
        "        if op == 'AND':\n",
        "            plist = postings.intersection(postings2)\n",
        "            return plist\n",
        "        elif op == 'OR':\n",
        "            plist = postings.union(postings2)\n",
        "            return plist\n",
        "        elif op == 'NOT':\n",
        "            if not NOT_switch: # used to control the order of postings and postings2\n",
        "                postings_copy = copy.deepcopy(postings)\n",
        "                for i in postings2:\n",
        "                    if i in postings_copy:\n",
        "                        postings_copy._postings.remove(i)\n",
        "                return postings_copy\n",
        "            else:\n",
        "                postings2_copy = copy.deepcopy(postings2)\n",
        "                for i in postings:\n",
        "                    if i in postings2_copy:\n",
        "                        postings2_copy._postings.remove(i)\n",
        "                return postings2_copy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUTEfdG69JF5"
      },
      "source": [
        "## Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQUqcrdlxGkM"
      },
      "source": [
        "def print_result(answer: PostingList, spellingCorrection=False):\n",
        "    if spellingCorrection: # for a better output\n",
        "        print()\n",
        "    for movie in answer:\n",
        "        print(movie)\n",
        "\n",
        "def and_query(ir: IRsystem, text: str, spellingCorrection=False, noprint=True):\n",
        "    words = text.split()\n",
        "    answer = ir.answer_and_query(words, spellingCorrection)  # list of documents\n",
        "    if not noprint:\n",
        "        print_result(answer, spellingCorrection)\n",
        "    return answer\n",
        "        \n",
        "def or_query(ir: IRsystem, text: str, spellingCorrection=False, noprint=True):\n",
        "    words = text.split()\n",
        "    answer = ir.answer_or_query(words, spellingCorrection)\n",
        "    if not noprint:\n",
        "        print_result(answer, spellingCorrection)\n",
        "    return answer\n",
        "\n",
        "def not_query(ir: IRsystem, text: str, spellingCorrection=False, noprint=True):\n",
        "    words = text.split()\n",
        "    answer = ir.answer_not_query(words, spellingCorrection)\n",
        "    if not noprint:\n",
        "        print_result(answer, spellingCorrection)\n",
        "    return answer\n",
        "\n",
        "def query(ir: IRsystem, text: str, spellingCorrection=False, noprint=True):\n",
        "    \"\"\" This query can answer complex queries with 'AND', 'OR' and 'NOT' but without parentheses.\n",
        "    E.g. text = \"yoda AND darth OR Gandalf NOT love\"\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        print(\"You cannot use one single word! Use at least two words connected with a logical operator.\")\n",
        "        return None\n",
        "    for i, w in enumerate(words):\n",
        "        if w in ['AND', 'OR', 'NOT']:\n",
        "            if i == 1:\n",
        "                plist = ir.answer_query(op = w, words = [words[i-1], words[i+1]], spellingCorrection=spellingCorrection)\n",
        "            else:\n",
        "                plist = ir.answer_query(op = w, word = words[i+1], postings = plist, spellingCorrection=spellingCorrection)\n",
        "    answer = ir.get_from_corpus(plist)\n",
        "    if not noprint:\n",
        "        print_result(answer, spellingCorrection)\n",
        "    return answer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1c4zwaxBwWQ"
      },
      "source": [
        "def query_with_pars(ir: IRsystem, text: str, spellingCorrection=False, noprint=True):\n",
        "    \"\"\" This query can answer to any type of query, also complex ones. Use 'AND', 'OR' and 'NOT'\n",
        "    and parenthesis to specify how to combine the words in the query.\n",
        "    E.g. text = \"(yoda AND darth) OR Gandalf NOT love\"\n",
        "    \"\"\"\n",
        "    # add a space after '(' and before ')' so to split them into separate tokens\n",
        "    text = re.sub(r\"\\(\", r'( ', text)\n",
        "    text = re.sub(r\"\\)\", r' )', text)\n",
        "    # split the text in words (it eliminates spaces at the beginning/end of words, so even if we add them with the first passage it's not important)\n",
        "    words = text.split()\n",
        "\n",
        "    if len(words) == 1 or (len(words) == 3 and (\"(\" in words or \")\" in words)):\n",
        "        print(\"You cannot use one single word! Use at least two words connected with a logical operator.\")\n",
        "        return None\n",
        "\n",
        "    # words_mod = copy.deepcopy(words) # For now not necessary, I don't reuse the array 'words'\n",
        "    words_mod = words                  # I write it like this so it is easy to change is 'words' will be needed\n",
        "    # Wrap the text in two parentheses so to perform the following while one last time, avoiding to repeat code, and compute the whole query\n",
        "    words_mod.insert(0, \"(\")\n",
        "    words_mod.append(\")\")\n",
        "    \n",
        "    openp = []\n",
        "    closep = []\n",
        "    for i, w in enumerate(words_mod):\n",
        "        if w == \"(\": openp.append(i)\n",
        "        elif w == \")\": closep.append(i)\n",
        "    \n",
        "    if len(openp) != len(closep):\n",
        "        print(\"The number of open parentheses is different from the number of closed parentheses.\")\n",
        "        return None\n",
        "\n",
        "    while closep:\n",
        "        c = closep[0]\n",
        "        o = None\n",
        "        for i in openp:\n",
        "            if i < c:\n",
        "                o = i\n",
        "        #plist = PostingList()  # wrong, since if there are extra parentheses around all the query I overwrite the last plist and I store in words_mod an empty plist\n",
        "        # Process the query in these parenthesis, the remove o in openp and c in closep\n",
        "        for i, w in enumerate(words_mod[o+1 : c]):\n",
        "            if w in ['AND', 'OR', 'NOT']:\n",
        "                if i == 1:\n",
        "                    item1 = words_mod[(o+1) + i-1]\n",
        "                    item2 = words_mod[(o+1) + i+1]\n",
        "                    # Checks to call `ir.answer_query` in the right way\n",
        "                    if type(item1) == str and type(item2) == str:\n",
        "                        plist = ir.answer_query(op = w, words = [item1, item2], spellingCorrection=spellingCorrection)\n",
        "                    elif type(item1) == PostingList and type(item2) == str:\n",
        "                        plist = ir.answer_query(op = w, word = item2, postings = item1, spellingCorrection=spellingCorrection)\n",
        "                    elif type(item1) == str and type(item2) == PostingList:\n",
        "                        plist = ir.answer_query(op = w, word = item1, postings = item2, NOT_switch = True, spellingCorrection = spellingCorrection)\n",
        "                    elif type(item1) == PostingList and type(item2) == PostingList:\n",
        "                        plist = ir.answer_query(op = w, postings = item1, postings2 = item2, spellingCorrection=spellingCorrection)\n",
        "                else:\n",
        "                    item2 = words_mod[(o+1) + i+1]\n",
        "                    if type(item2) == str:\n",
        "                        plist = ir.answer_query(op = w, word = item2, postings = plist, spellingCorrection=spellingCorrection)\n",
        "                    elif type(item2) == PostingList:\n",
        "                        plist = ir.answer_query(op = w, postings = plist, postings2 = item2, spellingCorrection=spellingCorrection)\n",
        "        words_mod[o] = plist\n",
        "        del words_mod[o+1:c+1]\n",
        "        openp = []\n",
        "        closep = []\n",
        "        for i, w in enumerate(words_mod):\n",
        "            if w == \"(\": openp.append(i)\n",
        "            elif w == \")\": closep.append(i)\n",
        "    \n",
        "    answer = ir.get_from_corpus(plist)\n",
        "    if not noprint:\n",
        "        print_result(answer, spellingCorrection)\n",
        "    return answer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-7W9Qcn-nqd"
      },
      "source": [
        "## Phrase queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAg5voZx-X1O"
      },
      "source": [
        "## Wildcard queries\n",
        "\n",
        "A subtree of a tree T is a tree S consisting of a node in T and all of its descendants in T."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmkOEhk1xGkP"
      },
      "source": [
        "## Test queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6fkN6g6YhqQ"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2th9vocxGks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84937184-d67e-4e10-a3b7-c5443d05874a"
      },
      "source": [
        "corpus = read_movie_descriptions()\n",
        "len(corpus)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EYyh2C-3Wxf"
      },
      "source": [
        "#### Saving / loading the index\n",
        "\n",
        "We will save the index using `Pickle`. `Pickle` is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltb1WbmbACkP",
        "outputId": "a2aec48b-e14c-4a77-fab9-5500dad0828d"
      },
      "source": [
        "updated = True\n",
        "\n",
        "filename = \"index.pickle\"\n",
        "\n",
        "# If the index is saved and it is updated I load it, otherwise I create it and save it\n",
        "if os.path.isfile(filename) and updated:\n",
        "    print (\"Index file exists. Loading the index...\")\n",
        "    # load the index\n",
        "    tic = time.time()\n",
        "    infile = open(filename, 'rb')\n",
        "    idx = pickle.load(infile)\n",
        "    infile.close()\n",
        "    toc = time.time()\n",
        "    print(\"Index loaded.\")\n",
        "    print(f\"Time: {round(toc-tic, 3)}s\")\n",
        "else:\n",
        "    print (\"Index file does not exist.\")\n",
        "    tic = time.time()\n",
        "    idx = InvertedIndex.from_corpus(corpus)\n",
        "    toc = time.time()\n",
        "    print(f\"\\n\\nTime: {round(toc-tic, 3)}s\")\n",
        "    # save the index\n",
        "    outfile = open(filename, 'wb')\n",
        "    pickle.dump(idx, outfile)\n",
        "    outfile.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index file exists. Loading the index...\n",
            "Index loaded.\n",
            "Time: 13.297s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6vKVTvKxGku",
        "outputId": "a17fda4a-5227-48e6-efbe-736c88572b84"
      },
      "source": [
        "print(idx)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A dictionary with 194757 terms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6RjfRZDxGkz"
      },
      "source": [
        "ir = IRsystem(corpus, idx)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMyIwBQPOoe4",
        "outputId": "06825f5a-2365-4935-e249-a284c2b91faf"
      },
      "source": [
        "try:\n",
        "  ir.get_from_corpus(ir._index[normalize(\"thig\")])\n",
        "except KeyError:\n",
        "    print(sys.exc_info()[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"The term 'thig' is not present in the index.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pbRmqgN4gRv"
      },
      "source": [
        "### AND queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV03aYgCxGkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d23d2-e6c0-4efd-9bb0-3ccf331d46fd"
      },
      "source": [
        "fg_and_query = and_query(ir, \"frodo Gandalf\", noprint=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Date Movie\n",
            "The Lord of the Rings: The Two Towers\n",
            "The Lord of the Rings: The Return of the King\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahR0YEwgxGk0",
        "outputId": "1125dde7-8ad0-4b06-ba38-eb2d652eb376"
      },
      "source": [
        "yld_and_query = and_query(ir, \"yoda Luke darth\", noprint=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Something, Something, Something Dark Side\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "It's a Trap!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgIQuYe7xGk2"
      },
      "source": [
        "frodo_query = ir.get_from_corpus(ir._index[normalize(\"frodo\")])\n",
        "frodo_set = set(frodo_query)\n",
        "\n",
        "gandalf_query = ir.get_from_corpus(ir._index[normalize(\"Gandalf\")])\n",
        "gandalf_set = set(gandalf_query)\n",
        "\n",
        "fg_and_set = frodo_set.intersection(gandalf_set)\n",
        "\n",
        "assert set(fg_and_query) == fg_and_set"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9epvCZy9Hyo1"
      },
      "source": [
        "yoda_query = ir.get_from_corpus(ir._index[normalize(\"yoda\")])\n",
        "yoda_set = set(yoda_query)\n",
        "\n",
        "luke_query = ir.get_from_corpus(ir._index[normalize(\"Luke\")])\n",
        "luke_set = set(luke_query)\n",
        "\n",
        "darth_query = ir.get_from_corpus(ir._index[normalize(\"darth\")])\n",
        "darth_set = set(darth_query)\n",
        "\n",
        "yld_and_set = yoda_set.intersection(luke_set).intersection(darth_set)\n",
        "\n",
        "assert set(yld_and_query) == yld_and_set"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R51dLb6x-cZ"
      },
      "source": [
        "#### With spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmkt_mwx9CF",
        "outputId": "80e814b8-b08a-4662-fa18-195d04202f69"
      },
      "source": [
        "mispelled_and_query = and_query(ir, \"yioda lukke darhth\", spellingCorrection=True, noprint=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yioda not found. Did you mean yoda?\n",
            "lukke not found. Did you mean luke?\n",
            "darhth not found. Did you mean darth?\n",
            "\n",
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Something, Something, Something Dark Side\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "It's a Trap!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVlCeOtj8eU2"
      },
      "source": [
        "assert yld_and_query == mispelled_and_query"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71S0x5C44jbL"
      },
      "source": [
        "### OR queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvgjTQIVxGk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0e0dfe-054e-47df-d50b-4d7eccb10437"
      },
      "source": [
        "fy_or_query = or_query(ir, \"frodo yoda\", noprint=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "Something, Something, Something Dark Side\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Star Wars: The Clone Wars\n",
            "Date Movie\n",
            "Gulliver's Travels\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ6JWuDhxGk2"
      },
      "source": [
        "fy_or_set = set(frodo_query + yoda_query)\n",
        "\n",
        "assert set(fy_or_query) == fy_or_set"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LZkIiLrL1gy",
        "outputId": "909f12f5-b338-476c-9e32-a50cc2942880"
      },
      "source": [
        "fyg_or_query = or_query(ir, \"frodo yoda gandalf\", noprint=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Imaginationland Episode II\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "Something, Something, Something Dark Side\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Star Wars: The Clone Wars\n",
            "Date Movie\n",
            "Gulliver's Travels\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzvw6iQuxGk3"
      },
      "source": [
        "fyg_or_set = set(frodo_query + yoda_query + gandalf_query)\n",
        "\n",
        "assert set(fyg_or_query) == fyg_or_set"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTonia6mxGk4"
      },
      "source": [
        "love_query = ir.get_from_corpus(ir._index[normalize(\"love\")])\n",
        "fyl_or_query = or_query(ir, \"frodo yoda love\")\n",
        "fyl_or_set = set(frodo_query + yoda_query + love_query)\n",
        "\n",
        "assert set(fyl_or_query) == fyl_or_set"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjfy6ezpO6Bs"
      },
      "source": [
        "#### With spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WM_P2qtO6t6",
        "outputId": "0218661f-4998-4e90-d9da-5fe8dca95b25"
      },
      "source": [
        "mispelled_or_query = or_query(ir, \"frodoo yioda ganalf\", spellingCorrection=True, noprint=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frodoo not found. Did you mean frodo?\n",
            "yioda not found. Did you mean yoda?\n",
            "ganalf not found. Did you mean gandalf?\n",
            "\n",
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Imaginationland Episode II\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "Something, Something, Something Dark Side\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Star Wars: The Clone Wars\n",
            "Date Movie\n",
            "Gulliver's Travels\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mbsgw1iPHPQ"
      },
      "source": [
        "assert fyg_or_query == mispelled_or_query"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THIMgdQcYT9d"
      },
      "source": [
        "### NOT queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUz7TIfok6pA"
      },
      "source": [
        "a_not_query = not_query(ir, \"a\", noprint=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtYbRBF1VuNT"
      },
      "source": [
        "corpus_set = set(corpus)\n",
        "a_query = ir.get_from_corpus(ir._index[normalize(\"a\")])\n",
        "a_set = set(a_query)\n",
        "a_not_set = corpus_set.difference(a_set)\n",
        "\n",
        "assert set(a_not_query) == a_not_set"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2swuAGFm4UOT"
      },
      "source": [
        "lm_not_query = not_query(ir, \"love mother\", noprint=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUbAlgOTYWDc"
      },
      "source": [
        "love_set = set(love_query)\n",
        "mother_query = ir.get_from_corpus(ir._index[normalize(\"mother\")])\n",
        "mother_set = set(mother_query)\n",
        "lm_set = love_set.union(mother_set)\n",
        "lm_not_set = corpus_set.difference(lm_set)\n",
        "\n",
        "assert set(lm_not_query) == lm_not_set"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVKdhWepxPAr"
      },
      "source": [
        "yg_not_query = not_query(ir, \"yoda Gandalf\", noprint=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_RQIFpdxRIs"
      },
      "source": [
        "yg_set = yoda_set.union(gandalf_set)\n",
        "yg_not_set = corpus_set.difference(yg_set)\n",
        "\n",
        "assert set(yg_not_query) == yg_not_set"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE7TYL67zHgA"
      },
      "source": [
        "#### With spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeYh5PIeyB34",
        "outputId": "f5f97e79-dde3-4d37-9437-9618f1d05f49"
      },
      "source": [
        "mispelled_a_not_query = not_query(ir, \"aq\", spellingCorrection=True, noprint=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aq not found. Did you mean a?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx_3UtF2yitW"
      },
      "source": [
        "assert a_not_query == mispelled_a_not_query"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkXx1W2tX037"
      },
      "source": [
        "### Compex queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAKDAvDKCP_H",
        "outputId": "a9409019-769e-45ed-e289-1160beba1e3b"
      },
      "source": [
        "query(ir, \"yoda\", noprint=False)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You cannot use one single word! Use at least two words connected with a logical operator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21oBBs-O3gSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b6d4ad-959d-446d-b858-0da76a71b620"
      },
      "source": [
        "yAdOg_query = query(ir, \"yoda AND darth OR Gandalf\", noprint=False)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Imaginationland Episode II\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "Something, Something, Something Dark Side\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Date Movie\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmEhcAFy2G2E"
      },
      "source": [
        "yd_and_set = yoda_set.intersection(darth_set)\n",
        "yAdOg_set = yd_and_set.union(gandalf_set)\n",
        "\n",
        "assert set(yAdOg_query) == yAdOg_set"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFn1hry8hbrA",
        "outputId": "d91acbc3-9b72-4930-c0d3-c2b6668e3189"
      },
      "source": [
        "yAdOg_query2 = query_with_pars(ir, \"yoda AND darth OR Gandalf\", noprint=False)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Imaginationland Episode II\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "Something, Something, Something Dark Side\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Date Movie\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH36pxNohhjs"
      },
      "source": [
        "assert set(yAdOg_query2) == yAdOg_set"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-x1O0vuYuXB"
      },
      "source": [
        "yOdAg_query = query(ir, \"yoda OR darth AND Gandalf\", noprint=False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuT4Y8I-Y1jT"
      },
      "source": [
        "yd_or_set = yoda_set.union(darth_set)\n",
        "yOdAg_set = yd_or_set.intersection(gandalf_set)\n",
        "\n",
        "assert set(yOdAg_query) == yOdAg_set"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9DHlAUpazQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87354e8-e5a0-4f4d-fe7a-ffc20280760c"
      },
      "source": [
        "yOdOgAl_query = query(ir, \"yoda OR darth OR Gandalf AND love\", noprint=False)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "Wishology\n",
            "Something, Something, Something Dark Side\n",
            "Date Movie\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "The Lord of the Rings: The Return of the King\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I6hwNlYbCsF"
      },
      "source": [
        "ydg_and_set = yoda_set.union(darth_set).union(gandalf_set)\n",
        "yOdOgAl_set = ydg_and_set.intersection(love_set)\n",
        "\n",
        "assert set(yOdOgAl_query) == yOdOgAl_set"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm1eBPk65RT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441c2a6b-2d02-44c9-9b80-0a0afb2be5ae"
      },
      "source": [
        "yAdOgNl_query = query(ir, \"yoda AND darth OR Gandalf NOT love\", noprint=False)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imaginationland Episode II\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "It's a Trap!\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQZtMLMI5TAs"
      },
      "source": [
        "yAdOgNl_set = yAdOg_set.difference(love_set)\n",
        "\n",
        "assert set(yAdOgNl_query) == yAdOgNl_set"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG4t5XVT-_sa",
        "outputId": "f8254f91-4866-4127-dc84-0abb34f640b4"
      },
      "source": [
        "yNdOg_query = query(ir, \"yoda NOT darth OR Gandalf\", noprint=False)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imaginationland Episode II\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars: The Clone Wars\n",
            "Date Movie\n",
            "Gulliver's Travels\n",
            "The Lord of the Rings: The Two Towers\n",
            "The Lord of the Rings: The Return of the King\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwC9LxN-_Kxq"
      },
      "source": [
        "yNd_set = yoda_set.difference(darth_set)\n",
        "yNdOg_set = yNd_set.union(gandalf_set)\n",
        "\n",
        "assert set(yNdOg_query) == yNdOg_set"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixxPBxsQ1XYw"
      },
      "source": [
        "#### Using parentheses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5ZLonY0CWkd",
        "outputId": "1f3f85e3-ffb6-42ca-f73f-9f98e9d19c1d"
      },
      "source": [
        "query_with_pars(ir, \"(yoda)\", noprint=False)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You cannot use one single word! Use at least two words connected with a logical operator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mKg98aJX0F2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb3867e-9c43-4e43-ca76-b93c8d16a216"
      },
      "source": [
        "pyAdpOgNl_query = query_with_pars(ir, \"(yoda AND darth) OR Gandalf NOT love\", noprint=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imaginationland Episode II\n",
            "George Lucas in Love\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "It's a Trap!\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wwpRQUmi5Ow"
      },
      "source": [
        "assert set(pyAdpOgNl_query) == yAdOgNl_set"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd7hmB2gollK",
        "outputId": "904a7956-99e4-4c18-c726-7254950d17b8"
      },
      "source": [
        "yApdOgpNl_query = query_with_pars(ir, \"yoda AND (darth OR Gandalf) NOT love\", noprint=False)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George Lucas in Love\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "It's a Trap!\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX235c-tsvGt"
      },
      "source": [
        "dOg_set = darth_set.union(gandalf_set)\n",
        "yApdOgpNl = yoda_set.intersection(dOg_set).difference(love_set)\n",
        "\n",
        "assert set(yApdOgpNl_query) == yApdOgpNl"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mmsBtidP3Bi",
        "outputId": "682f078a-b58f-4a25-e2ff-68bcdc8c0f7b"
      },
      "source": [
        "yOgApdOlp_complex_query = query_with_pars(ir, \"(yoda OR Gandalf AND (darth OR love))\", noprint=False)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "Something, Something, Something Dark Side\n",
            "Return of the Ewok\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Date Movie\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_UkW_cutlNm"
      },
      "source": [
        "dOl_set = darth_set.union(love_set)\n",
        "yOgApdNlp_set = yoda_set.union(gandalf_set).intersection(dOl_set)\n",
        "\n",
        "assert set(yOgApdOlp_complex_query) == yOgApdNlp_set"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC_UOXlOyfkk",
        "outputId": "04f3cfa6-fe01-40e2-f301-d3c67b6d6f67"
      },
      "source": [
        "yOpgApdOlpNmpOphNap_complex_query = query_with_pars(ir, \"yoda OR (Gandalf AND (darth OR love) NOT mother) OR (hello NOT a)\", noprint=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Bimbo's Express\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "Touchstone: Dancing With Angels\n",
            "Something, Something, Something Dark Side\n",
            "Return of the Ewok\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Star Wars: The Clone Wars\n",
            "Gulliver's Travels\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0OU2i6TytgM"
      },
      "source": [
        "gApdOlpNm_set = gandalf_set.intersection(dOl_set).difference(mother_set)\n",
        "hello_set = set(ir.get_from_corpus(ir._index[normalize(\"hello\")]))\n",
        "hNa_set = hello_set.difference(a_set)\n",
        "yOpgApdOlpNmpOphNap_set = yoda_set.union(gApdOlpNm_set).union(hNa_set)\n",
        "\n",
        "assert set(yOpgApdOlpNmpOphNap_complex_query) == yOpgApdOlpNmpOphNap_set"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r253vxhbbNq",
        "outputId": "431854ac-1998-4d4a-c9c3-9a05bb333f23"
      },
      "source": [
        "test = \"hello OR ((how AND (are OR you) OR I AND (am AND fine) OR I) AND am AND (sleepy OR hungry) AND cold)\"\n",
        "hOphApaOypOiApaAfpOiAaApsOhpAcp_query = query_with_pars(ir, test, noprint=False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dark Water\n",
            "House\n",
            "Pantry Panic\n",
            "The Jazz Singer\n",
            "A Witch's Tangled Hare\n",
            "Look Who's Talking Too\n",
            "The Life of Reilly\n",
            "Bimbo's Express\n",
            "Hansel and Gretel\n",
            "Phone Booth\n",
            "Queen of the Damned\n",
            "Ghost Town\n",
            "Ghosts\n",
            "Touchstone: Dancing With Angels\n",
            "The Weather Man\n",
            "The Friendly Ghost\n",
            "Say Hello to Yesterday\n",
            "Grave Encounters\n",
            "Duets\n",
            "The Strangers\n",
            "Hello Dolly!\n",
            "Martian Through Georgia\n",
            "The Daffy Doc\n",
            "A Star Is Born\n",
            "Heartlands\n",
            "Crazy People\n",
            "WALL-E\n",
            "A Star Is Born\n",
            "Sweet Home Alabama\n",
            "Takeshis'\n",
            "Closer\n",
            "Old School\n",
            "The Nutcracker Prince\n",
            "Jazzin' for Blue Jean\n",
            "Prom Night II\n",
            "The Day After\n",
            "Bugs and Thugs\n",
            "I Could Go On Singing\n",
            "Din of Celestial Birds\n",
            "A Fine Feathered Frenzy\n",
            "One False Move\n",
            "Psychomania\n",
            "Trapped in the Closet Chapters 13–22\n",
            "Easy Money\n",
            "Atlantic Rhapsody\n",
            "Love Bites\n",
            "Tom and Cherie\n",
            "Sleepless in Seattle\n",
            "Yummy Yummy\n",
            "The Adventure of Iron Pussy\n",
            "Tetsuo: The Iron Man\n",
            "An Affair to Remember\n",
            "Motel Hell\n",
            "I'm Here\n",
            "28 Days Later\n",
            "Jerry Maguire\n",
            "The Hole\n",
            "Love Affair\n",
            "Basil\n",
            "The Stepford Wives\n",
            "Wake Up Jeff\n",
            "Claire Dolan\n",
            "The In-Laws\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVwFGZyb2zrR"
      },
      "source": [
        "are_set = set(ir.get_from_corpus(ir._index[normalize(\"are\")]))\n",
        "you_set = set(ir.get_from_corpus(ir._index[normalize(\"you\")]))\n",
        "aOy_set = are_set.union(you_set)\n",
        "am_set = set(ir.get_from_corpus(ir._index[normalize(\"am\")]))\n",
        "aAf_set = am_set.intersection(set(ir.get_from_corpus(ir._index[normalize(\"fine\")])))\n",
        "i_set = set(ir.get_from_corpus(ir._index[normalize(\"I\")]))\n",
        "how_set = set(ir.get_from_corpus(ir._index[normalize(\"how\")]))\n",
        "hApaOypOiApaAfpOi_set = how_set.intersection(aOy_set).union(i_set).intersection(aAf_set).union(i_set)\n",
        "sleepy_set = set(ir.get_from_corpus(ir._index[normalize(\"sleepy\")]))\n",
        "hungry_set = set(ir.get_from_corpus(ir._index[normalize(\"hungry\")]))\n",
        "sOh_set = sleepy_set.union(hungry_set)\n",
        "cold_set = set(ir.get_from_corpus(ir._index[normalize(\"cold\")]))\n",
        "hApaOypOiApaAfpOiAaApsOhpAc_set = hApaOypOiApaAfpOi_set.intersection(am_set).intersection(sOh_set).intersection(cold_set)\n",
        "hOphApaOypOiApaAfpOiAaApsOhpAcp_set = hello_set.union(hApaOypOiApaAfpOiAaApsOhpAc_set)\n",
        "\n",
        "assert set(hOphApaOypOiApaAfpOiAaApsOhpAcp_query) == hOphApaOypOiApaAfpOiAaApsOhpAcp_set"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oakzfQ5tumuj"
      },
      "source": [
        "#### With spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thZSJ10uzRIo",
        "outputId": "66d3dc7f-8640-4c85-b552-357092e73305"
      },
      "source": [
        "mispelled_yNdOg_query = query(ir, \"yioda NOT ddarth OR Ganalf\", spellingCorrection=True, noprint=False)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yioda not found. Did you mean yoda?\n",
            "ddarth not found. Did you mean darth?\n",
            "ganalf not found. Did you mean gandalf?\n",
            "\n",
            "Imaginationland Episode II\n",
            "The Lord of the Rings: The Fellowship of the Ring\n",
            "The Lord of the Rings\n",
            "The Hunt for Gollum\n",
            "The Return of the King\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars: The Clone Wars\n",
            "Date Movie\n",
            "Gulliver's Travels\n",
            "The Lord of the Rings: The Two Towers\n",
            "The Lord of the Rings: The Return of the King\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irQkYrJuuqo9"
      },
      "source": [
        "assert yNdOg_query == mispelled_yNdOg_query"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Fi6pyGHxLR",
        "outputId": "be646b25-8461-43f6-cebd-2bba80711c86"
      },
      "source": [
        "mispelled_yOpgApdOlpNmpOphNap_complex_query = query_with_pars(ir, \"yioda OR (Ganalf AND (ddarth OR lovve) NOT motther) OR (helloo NOT aq)\", spellingCorrection=True, noprint=False)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ddarth not found. Did you mean darth?\n",
            "lovve not found. Did you mean love?\n",
            "ganalf not found. Did you mean gandalf?\n",
            "motther not found. Did you mean mother?\n",
            "helloo not found. Did you mean hello?\n",
            "aq not found. Did you mean a?\n",
            "yioda not found. Did you mean yoda?\n",
            "\n",
            "Star Wars Episode V: The Empire Strikes Back\n",
            "Bimbo's Express\n",
            "Star Wars Episode II: Attack of the Clones\n",
            "George Lucas in Love\n",
            "Touchstone: Dancing With Angels\n",
            "Something, Something, Something Dark Side\n",
            "Return of the Ewok\n",
            "Aliens in the Wild, Wild West\n",
            "Star Wars Episode III: Revenge of the Sith\n",
            "Star Wars Episode VI: Return of the Jedi\n",
            "Star Wars: The Clone Wars\n",
            "Gulliver's Travels\n",
            "Lego Star Wars: The Quest for R2-D2\n",
            "The Lord of the Rings: The Two Towers\n",
            "It's a Trap!\n",
            "The Lord of the Rings: The Return of the King\n",
            "LEGO Star Wars: Revenge of the Brick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGU61YiTH7Aw"
      },
      "source": [
        "assert yOpgApdOlpNmpOphNap_complex_query == mispelled_yOpgApdOlpNmpOphNap_complex_query"
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}